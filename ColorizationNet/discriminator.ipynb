{"cells":[{"cell_type":"code","source":["import os\n","import cv2\n","import math\n","import torch\n","import shutil \n","import random\n","import numpy as np \n","import pandas as pd \n","from PIL import Image\n","import torch.nn as nn\n","from torch import cuda\n","from torch.optim import Adam\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, Dataset\n","from skimage.color import rgb2lab, lab2rgb, rgb2gray\n","import torchvision.models as models\n","from tqdm import tqdm"],"metadata":{"id":"ZjqVQGwn-aI6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os, time, shutil, argparse\n","import pandas as pd\n","from functools import partial\n","import pickle\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.models as models\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.models as models\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from skimage.color import lab2rgb\n","from skimage import io\n","import torch.optim as optim"],"metadata":{"id":"-rODWW-A_E0D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","import shutil\n","def save_model(model):\n","  drive.mount('/content/drive')\n","  shutil.copy(\"/content/\" + model, \"/content/drive/MyDrive/cv thesis/model\")\n","  print(\"Model Saved\")\n","  drive.flush_and_unmount()"],"metadata":{"id":"BIT3PNkqAI5-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IbxpB5IE0wxW"},"outputs":[],"source":["!pip install --upgrade --no-cache-dir gdown\n","!gdown --id 1Csq7o2JqQjUkmPM4JPFwPIA74-_ik5Mt #Sample 12k imagenet\n","!gdown --id 1smp7oD7RftKQCyQrlL7Jf3T0OGg8UURl #Caption file - sampled.csv\n","!unzip images.zip"]},{"cell_type":"code","source":["# !gdown --id 1omdfiq5ijDIskNjPepir3mxMaNV3twgc #DAVIS dataset\n","# !unzip DAVIS.zip"],"metadata":{"id":"9A5AzWseNJ4a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !gdown --id 1YHdTCB4eMCbn4TAPtrdgIJLYF5cC9rt4 #Kota dataset\n","# !unzip kota.zip"],"metadata":{"id":"KCXla4ztZraB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P681HoHIRRSz","executionInfo":{"status":"ok","timestamp":1650022103047,"user_tz":-330,"elapsed":4,"user":{"displayName":"Harsh Vardhan Bhadauriya","userId":"01145987489048234126"}},"outputId":"2e980e83-f5e2-482d-9fd2-cf427f791d6d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["images\timages.zip  sample_data  sampled.csv\n"]}]},{"cell_type":"code","source":["!gdown --id 1-zdr4QV1_-OQxQ-URN2zb1JNojHm3pyO #resnet gray weight\n","# # !gdown --id 1-jFyCtViLuwmRlfepHKKIdgFjvDvPlTS #epoch trained weight\n","# !gdown --id 1HM1fSJ4JhRGcBoODbmSbFpWkUXEkLg9f #Finetuned model epoch 2\n","# !gdown --id 1-7GOmJvjcfx4HSUV5NLUVRcJpvlLPjI9 #Finetuned model epoch 5\n","!gdown --id 1-k5sNFrq7K1ZSfbxIHGDTtSLtgCxY310 #Finetuned generator model epoch 10"],"metadata":{"id":"K8hwiGccPYZU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ImageDataset(Dataset):\n","    def __init__(self, root, captions_file, color_transform = None, transform = None):\n","        self.df = pd.read_csv(captions_file, index_col=None)\n","        self.transform = transform\n","        self.color_transform = color_transform\n","        self.images = self.df[\"image\"]        \n","        self.root = root\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index):\n","        img = Image.open(os.path.join(self.root , self.images[index])).convert(\"RGB\")\n","        if self.color_transform:\n","          img = self.color_transform(img)\n","        img = np.array(img)\n","        \n","        img_lab = rgb2lab(img).astype(\"float32\")\n","        lab_scaled =  (img_lab + 128) / 255\n","        \n","\n","        L = rgb2gray(img)\n","        ab = lab_scaled[: , : , 1:]\n","        # print(L.shape)\n","        if self.transform:\n","            L = self.transform(L)\n","            ab = self.transform(ab)\n","\n","        return L , ab"],"metadata":{"id":"XW_ociJJOt0E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ColorizationNet(nn.Module):\n","    def __init__(self, midlevel_input_size=128, global_input_size=512):\n","        super(ColorizationNet, self).__init__()\n","        # Fusion layer to combine midlevel and global features\n","        self.midlevel_input_size = midlevel_input_size\n","        self.global_input_size = global_input_size\n","        self.fusion = nn.Linear(midlevel_input_size + global_input_size, midlevel_input_size)\n","        self.bn1 = nn.BatchNorm1d(midlevel_input_size)\n","\n","        # Convolutional layers and upsampling\n","        self.deconv1_new = nn.ConvTranspose2d(midlevel_input_size, 128, kernel_size=4, stride=2, padding=1)\n","        self.conv1 = nn.Conv2d(midlevel_input_size, 128, kernel_size=3, stride=1, padding=1)\n","        self.bn2 = nn.BatchNorm2d(128)\n","        self.conv2 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)\n","        self.bn3 = nn.BatchNorm2d(64)\n","        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n","        self.bn4 = nn.BatchNorm2d(64)\n","        self.conv4 = nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1)\n","        self.bn5 = nn.BatchNorm2d(32)\n","        self.conv5 = nn.Conv2d(32, 2, kernel_size=3, stride=1, padding=1)\n","        self.upsample = nn.Upsample(scale_factor=2)\n","\n","        print('Loaded colorization net.')\n","\n","    def forward(self, midlevel_input, global_input):\n","        \n","        # Convolutional layers and upsampling\n","        x = F.relu(self.bn2(self.conv1(midlevel_input)))\n","        x = self.upsample(x)\n","        x = F.relu(self.bn3(self.conv2(x)))\n","        x = F.relu(self.conv3(x))\n","        x = self.upsample(x)\n","        x = F.sigmoid(self.conv4(x))\n","        x = self.upsample(self.conv5(x))\n","        return x"],"metadata":{"id":"nq_teAR_5d3z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ColorNet(nn.Module):\n","    def __init__(self):\n","        super(ColorNet, self).__init__()\n","        \n","        # Build ResNet and change first conv layer to accept single-channel input\n","        resnet_gray_model = models.resnet18(num_classes=365)\n","        resnet_gray_model.conv1.weight = nn.Parameter(resnet_gray_model.conv1.weight.sum(dim=1).unsqueeze(1).data)\n","        \n","        # Only needed if not resuming from a checkpoint: load pretrained ResNet-gray model\n","        if torch.cuda.is_available(): # and only if gpu is available\n","            resnet_gray_weights = torch.load('/content/resnet_gray_weights.pth.tar') #torch.load('pretrained/resnet_gray.tar')['state_dict']\n","            resnet_gray_model.load_state_dict(resnet_gray_weights)\n","            print('Pretrained ResNet-gray weights loaded')\n","\n","        # Extract midlevel and global features from ResNet-gray\n","        self.midlevel_resnet = nn.Sequential(*list(resnet_gray_model.children())[0:6])\n","        self.global_resnet = nn.Sequential(*list(resnet_gray_model.children())[0:9])\n","        self.fusion_and_colorization_net = ColorizationNet()\n","\n","    def forward(self, input_image):\n","\n","        # Pass input through ResNet-gray to extract features\n","        midlevel_output = self.midlevel_resnet(input_image)\n","        global_output = self.global_resnet(input_image)\n","\n","        # Combine features in fusion layer and upsample\n","        output = self.fusion_and_colorization_net(midlevel_output, global_output)\n","        return output\n"],"metadata":{"id":"APpgOsTG6c0W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CNNBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride):\n","        super(CNNBlock, self).__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(\n","                in_channels, out_channels, 4, stride, 1, bias=False, padding_mode=\"reflect\"\n","            ),\n","            nn.InstanceNorm2d(out_channels, affine = True),\n","            nn.LeakyReLU(0.2),\n","        )\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, in_channels=4, features=[64, 128, 256, 512]):\n","        super().__init__()\n","        self.initial = nn.Sequential(\n","            nn.Conv2d(\n","                in_channels,\n","                features[0],\n","                kernel_size=4,\n","                stride=2,\n","                padding=1,\n","                padding_mode=\"reflect\",\n","            ),\n","            nn.LeakyReLU(0.2),\n","        )\n","\n","        layers = []\n","        in_channels = features[0]\n","        for feature in features[1:]:\n","            layers.append(\n","                CNNBlock(in_channels, feature, stride=1 if feature == features[-1] else 2),\n","            )\n","            in_channels = feature\n","\n","        layers.append(\n","            nn.Conv2d(\n","                in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"\n","            ),\n","        )\n","\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, x, y):\n","        x = torch.cat([x, y], dim=1)\n","        x = self.initial(x)\n","        x = self.model(x)\n","        return x"],"metadata":{"id":"3XZ-aeLnzPJs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["color_transform = transforms.Compose([\n","            transforms.Resize((300), Image.BICUBIC),\n","            transforms.CenterCrop(256),\n","\n","        ])\n","transform = transforms.Compose([\n","            transforms.ToTensor()\n","        ])\n","\n","dataset = ImageDataset(r\"/content/images\", r\"/content/sampled.csv\", color_transform, transform)\n","loader = DataLoader(dataset=dataset, batch_size = 64, num_workers = 0, shuffle = True, pin_memory = True, drop_last = False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JtCFPC0C-aww","executionInfo":{"status":"ok","timestamp":1650022267816,"user_tz":-330,"elapsed":383,"user":{"displayName":"Harsh Vardhan Bhadauriya","userId":"01145987489048234126"}},"outputId":"995a621a-a191-4053-fb42-6794250e02bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"]}]},{"cell_type":"code","source":["print(len(loader), len(loader.dataset))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-fd86yES_cHh","executionInfo":{"status":"ok","timestamp":1650022281344,"user_tz":-330,"elapsed":3,"user":{"displayName":"Harsh Vardhan Bhadauriya","userId":"01145987489048234126"}},"outputId":"a4662712-31bb-4dc3-b36d-86e8837035ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["188 12020\n"]}]},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using {device} device\")\n","# device = \"cpu\""],"metadata":{"id":"OeaXjTvdBJej","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650022285336,"user_tz":-330,"elapsed":361,"user":{"displayName":"Harsh Vardhan Bhadauriya","userId":"01145987489048234126"}},"outputId":"8c96d824-2dac-47fc-bd1e-c3c24d05daa7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda device\n"]}]},{"cell_type":"code","source":["net_D = Discriminator()\n","net_D.to(device)"],"metadata":{"id":"k290wh7FzZ6v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net_G = ColorNet()\n","# Use GPU if available\n","if device == \"cuda\":  \n","  net_G.to(device)    \n","  print('Loaded model onto GPU.') \n","\n","if os.path.exists(\"/content/netG_pretrained_fine_tune_10.pt\"):\n","  checkpoint = torch.load('/content/netG_pretrained_fine_tune_10.pt')\n","  net_G.load_state_dict(checkpoint)\n","  print(\"Pretrained Model loaded\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xv6uZhZSJ_R-","executionInfo":{"status":"ok","timestamp":1650022604511,"user_tz":-330,"elapsed":3,"user":{"displayName":"Harsh Vardhan Bhadauriya","userId":"01145987489048234126"}},"outputId":"d5f59f73-ca29-42f5-d40d-203b06d0373e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pretrained ResNet-gray weights loaded\n","Loaded colorization net.\n","Loaded model onto GPU.\n","Pretrained Model loaded\n"]}]},{"cell_type":"code","source":["BCE = nn.BCEWithLogitsLoss()\n","opt_disc = optim.Adam(net_D.parameters(), lr=3e-4, betas=(0.5, 0.999),)\n","d_scaler = torch.cuda.amp.GradScaler()"],"metadata":{"id":"AGm3smgz7f58"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    print('Starting validation.')    \n","    # Switch model to validation mode\n","    net_G.eval()\n","    net_D.train()\n","    for e in range(1, 11):  \n","      for (gray, ab) in tqdm(loader):\n","\n","          # Use GPU if available\n","          L = gray.to(device = device, dtype = torch.float32)\n","          ab = ab.to(device = device, dtype = torch.float32)\n","          out_ab = net_G(L) \n","\n","          fake = torch.cat([L, out_ab], dim = 1)\n","          color = torch.cat([L, ab], dim = 1)\n","\n","          D_real = net_D(L, color)\n","          D_fake = net_D(L, fake.detach()) \n","\n","          # Train Discriminator\n","          with torch.cuda.amp.autocast():\n","              D_real_loss = BCE(D_real, torch.ones_like(D_real))\n","              D_fake_loss = BCE(D_fake, torch.zeros_like(D_fake))\n","              D_loss = (D_real_loss + D_fake_loss) / 2\n","\n","          net_D.zero_grad()\n","          d_scaler.scale(D_loss).backward()\n","          d_scaler.step(opt_disc)\n","          d_scaler.update()\n","      print(f\"Epoch {e}\")\n","      print(f\"Loss: {D_loss/len(loader):.5f}\")  \n","      path = \"netD_no_gan_train\" + str(e) + \".pt\"\n","      print(\"Path :\", path)\n","      torch.save(net_D.state_dict(), \"/content/\" + path)\n","      save_model(path)"],"metadata":{"id":"eZ7EjVFlDblK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650027873491,"user_tz":-330,"elapsed":5262193,"user":{"displayName":"Harsh Vardhan Bhadauriya","userId":"01145987489048234126"}},"outputId":"6efbdf7b-b551-4a94-d29f-4f45ae4cb6bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting validation.\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/188 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","100%|██████████| 188/188 [08:30<00:00,  2.72s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1\n","Loss: 0.00338\n","Path : netD_no_gan_train1.pt\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Model Saved\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 188/188 [08:33<00:00,  2.73s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2\n","Loss: 0.00254\n","Path : netD_no_gan_train2.pt\n","Mounted at /content/drive\n","Model Saved\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 188/188 [08:34<00:00,  2.74s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3\n","Loss: 0.00190\n","Path : netD_no_gan_train3.pt\n","Mounted at /content/drive\n","Model Saved\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 188/188 [08:36<00:00,  2.75s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4\n","Loss: 0.00117\n","Path : netD_no_gan_train4.pt\n","Mounted at /content/drive\n","Model Saved\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 188/188 [08:34<00:00,  2.74s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5\n","Loss: 0.00154\n","Path : netD_no_gan_train5.pt\n","Mounted at /content/drive\n","Model Saved\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 188/188 [08:34<00:00,  2.74s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6\n","Loss: 0.00042\n","Path : netD_no_gan_train6.pt\n","Mounted at /content/drive\n","Model Saved\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 188/188 [08:31<00:00,  2.72s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7\n","Loss: 0.00055\n","Path : netD_no_gan_train7.pt\n","Mounted at /content/drive\n","Model Saved\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 188/188 [08:24<00:00,  2.68s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8\n","Loss: 0.00027\n","Path : netD_no_gan_train8.pt\n","Mounted at /content/drive\n","Model Saved\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 188/188 [08:13<00:00,  2.62s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9\n","Loss: 0.00040\n","Path : netD_no_gan_train9.pt\n","Mounted at /content/drive\n","Model Saved\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 188/188 [08:15<00:00,  2.64s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10\n","Loss: 0.00021\n","Path : netD_no_gan_train10.pt\n","Mounted at /content/drive\n","Model Saved\n"]}]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    print('Starting validation.')    \n","    # Switch model to validation mode\n","    net_G.eval()\n","    net_D.train()\n","    for e in range(11, 21):  \n","      for (gray, ab) in tqdm(loader):\n","\n","          # Use GPU if available\n","          L = gray.to(device = device, dtype = torch.float32)\n","          ab = ab.to(device = device, dtype = torch.float32)\n","          out_ab = net_G(L) \n","\n","          fake = torch.cat([L, out_ab], dim = 1)\n","          color = torch.cat([L, ab], dim = 1)\n","\n","          D_real = net_D(L, color)\n","          D_fake = net_D(L, fake.detach()) \n","\n","          # Train Discriminator\n","          with torch.cuda.amp.autocast():\n","              D_real_loss = BCE(D_real, torch.ones_like(D_real))\n","              D_fake_loss = BCE(D_fake, torch.zeros_like(D_fake))\n","              D_loss = (D_real_loss + D_fake_loss) / 2\n","\n","          net_D.zero_grad()\n","          d_scaler.scale(D_loss).backward()\n","          d_scaler.step(opt_disc)\n","          d_scaler.update()\n","      print(f\"Epoch {e}\")\n","      print(f\"Loss: {D_loss/len(loader):.5f}\")  \n","      path = \"netD_no_gan_train\" + str(e) + \".pt\"\n","      print(\"Path :\", path)\n","      torch.save(net_D.state_dict(), \"/content/\" + path)\n","      save_model(path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xTHiQfohVXHq","executionInfo":{"status":"ok","timestamp":1650033361724,"user_tz":-330,"elapsed":5327600,"user":{"displayName":"Harsh Vardhan Bhadauriya","userId":"01145987489048234126"}},"outputId":"9587e3c6-e6fc-4055-c147-709c0540284d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting validation.\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/188 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","100%|██████████| 188/188 [08:34<00:00,  2.74s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11\n","Loss: 0.00022\n","Path : netD_no_gan_train11.pt\n","Mounted at /content/drive\n","Model Saved\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 188/188 [08:33<00:00,  2.73s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12\n","Loss: 0.00020\n","Path : netD_no_gan_train12.pt\n","Mounted at /content/drive\n","Model Saved\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 188/188 [08:33<00:00,  2.73s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13\n","Loss: 0.00064\n","Path : netD_no_gan_train13.pt\n","Mounted at /content/drive\n","Model Saved\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 188/188 [08:32<00:00,  2.72s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14\n","Loss: 0.00011\n","Path : netD_no_gan_train14.pt\n","Mounted at /content/drive\n","Model Saved\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 188/188 [08:33<00:00,  2.73s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 15\n","Loss: 0.00022\n","Path : netD_no_gan_train15.pt\n","Mounted at /content/drive\n","Model Saved\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 188/188 [08:37<00:00,  2.75s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 16\n","Loss: 0.00020\n","Path : netD_no_gan_train16.pt\n","Mounted at /content/drive\n","Model Saved\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 188/188 [08:36<00:00,  2.75s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 17\n","Loss: 0.00016\n","Path : netD_no_gan_train17.pt\n","Mounted at /content/drive\n","Model Saved\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 188/188 [08:38<00:00,  2.76s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 18\n","Loss: 0.00007\n","Path : netD_no_gan_train18.pt\n","Mounted at /content/drive\n","Model Saved\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 188/188 [08:37<00:00,  2.75s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 19\n","Loss: 0.00020\n","Path : netD_no_gan_train19.pt\n","Mounted at /content/drive\n","Model Saved\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 188/188 [08:36<00:00,  2.75s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 20\n","Loss: 0.00006\n","Path : netD_no_gan_train20.pt\n","Mounted at /content/drive\n","Model Saved\n"]}]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    print('Starting validation.')    \n","    # Switch model to validation mode\n","    model.eval()\n","    \n","    for (gray, ab) in tqdm(val_loader):\n","\n","        # Use GPU if available\n","        L = gray.to(device = device, dtype = torch.float32)\n","        ab = ab.to(device = device, dtype = torch.float32)\n","        output = model(L) # throw away class predictions\n","        \n","        fake = torch.cat([L, output], dim = 1).detach().cpu().numpy()\n","        for i in range(fake.shape[0]):\n","          color_image = fake[i]\n","          color_image = color_image.transpose((1, 2, 0))\n","          color_image[:, :, 0:1] = color_image[:, :, 0:1] * 100\n","          color_image[:, :, 1:3] = color_image[:, :, 1:3] * 255 - 128\n","          color_image = lab2rgb(color_image.astype(np.float64))\n","          plt.axis(False)\n","          plt.imshow(color_image)\n","          plt.show()"],"metadata":{"id":"jejCcIUfMv9h"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"discriminator.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}